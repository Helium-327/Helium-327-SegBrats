{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于UNet3d进行改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "         )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 UNet_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3d_bn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3d_bn, self).__init__()\n",
    "        self.encoder1 = DoubleConv(in_channels, 32)\n",
    "        self.encoder2 = DoubleConv(32, 64)\n",
    "        self.encoder3 = DoubleConv(64, 128)\n",
    "        self.encoder4 = DoubleConv(128, 256)\n",
    "        self.encoder5 = DoubleConv(256, 512) \n",
    "\n",
    "        self.decoder1 = DoubleConv(512, 256)\n",
    "        self.conv_trans1 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(256, 128)\n",
    "        self.conv_trans2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv(128, 64)\n",
    "        self.conv_trans3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv(64, 32)\n",
    "        self.conv_trans4 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.out_conv = DoubleConv(32, out_channels)\n",
    "\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 编码器部分\n",
    "        t1 = self.encoder1(x)                                               # 32 x 128 x 128 x 128\n",
    "        out = F.max_pool3d(t1, 2, 2)                                        # 32 x 64 x 64 x 64\n",
    "                                    \n",
    "        t2 = self.encoder2(out)                                             # 64 x 64 x 64 x 64\n",
    "        out = F.max_pool3d(t2, 2, 2)                                        # 64 x 32 x 32 x 32\n",
    "        \n",
    "        t3 = self.encoder3(out)                                             # 128 x 32 x 32 x 32\n",
    "        out = F.max_pool3d(t3, 2, 2)                                        # 128 x 16 x 16 x 16\n",
    "        \n",
    "        t4 = self.encoder4(out)                                             # 256 x 16 x 16 x 16\n",
    "        out = F.max_pool3d(t4, 2, 2)                                        # 256 x 8 x 8 x 8\n",
    "        \n",
    "        out = self.encoder5(out)                                            # 512 x 8 x 8 x 8\n",
    "        \n",
    "        \n",
    "        \n",
    "        out = self.conv_trans1(out)                                         # 256 x 16 x 16 x 16\n",
    "        out = self.decoder1(torch.cat([out, t4], dim=1))                    # 256 x 16 x 16 x 16\n",
    "        \n",
    "        out = self.conv_trans2(out)                                          # 128 x 32 x 32 x 32\n",
    "        out = self.decoder2(torch.cat([out, t3], dim=1))                    # 128 x 32 x 32 x 32\n",
    "        \n",
    "        out = self.conv_trans3(out)                                         # 64 x 64 x 64 x 64\n",
    "        out = self.decoder3(torch.cat([out, t2], dim=1))                    # 64 x 64 x 64 x 64                \n",
    "\n",
    "        out = self.conv_trans4(out)                                         # 32 x 128 x 128 x 128\n",
    "        out = self.decoder4(torch.cat([out, t1], dim=1))                    # 32 x 128 x 128 x 128\n",
    "\n",
    "        out = self.out_conv(out)                                            # out_channels x 128 x 128\n",
    "        \n",
    "        out = self.soft(out)                                             # softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128, 128, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1     [1, 32, 128, 128, 128]           3,488\n",
      "       BatchNorm3d-2     [1, 32, 128, 128, 128]              64\n",
      "              ReLU-3     [1, 32, 128, 128, 128]               0\n",
      "            Conv3d-4     [1, 32, 128, 128, 128]          27,680\n",
      "       BatchNorm3d-5     [1, 32, 128, 128, 128]              64\n",
      "              ReLU-6     [1, 32, 128, 128, 128]               0\n",
      "        DoubleConv-7     [1, 32, 128, 128, 128]               0\n",
      "            Conv3d-8        [1, 64, 64, 64, 64]          55,360\n",
      "       BatchNorm3d-9        [1, 64, 64, 64, 64]             128\n",
      "             ReLU-10        [1, 64, 64, 64, 64]               0\n",
      "           Conv3d-11        [1, 64, 64, 64, 64]         110,656\n",
      "      BatchNorm3d-12        [1, 64, 64, 64, 64]             128\n",
      "             ReLU-13        [1, 64, 64, 64, 64]               0\n",
      "       DoubleConv-14        [1, 64, 64, 64, 64]               0\n",
      "           Conv3d-15       [1, 128, 32, 32, 32]         221,312\n",
      "      BatchNorm3d-16       [1, 128, 32, 32, 32]             256\n",
      "             ReLU-17       [1, 128, 32, 32, 32]               0\n",
      "           Conv3d-18       [1, 128, 32, 32, 32]         442,496\n",
      "      BatchNorm3d-19       [1, 128, 32, 32, 32]             256\n",
      "             ReLU-20       [1, 128, 32, 32, 32]               0\n",
      "       DoubleConv-21       [1, 128, 32, 32, 32]               0\n",
      "           Conv3d-22       [1, 256, 16, 16, 16]         884,992\n",
      "      BatchNorm3d-23       [1, 256, 16, 16, 16]             512\n",
      "             ReLU-24       [1, 256, 16, 16, 16]               0\n",
      "           Conv3d-25       [1, 256, 16, 16, 16]       1,769,728\n",
      "      BatchNorm3d-26       [1, 256, 16, 16, 16]             512\n",
      "             ReLU-27       [1, 256, 16, 16, 16]               0\n",
      "       DoubleConv-28       [1, 256, 16, 16, 16]               0\n",
      "           Conv3d-29          [1, 512, 8, 8, 8]       3,539,456\n",
      "      BatchNorm3d-30          [1, 512, 8, 8, 8]           1,024\n",
      "             ReLU-31          [1, 512, 8, 8, 8]               0\n",
      "           Conv3d-32          [1, 512, 8, 8, 8]       7,078,400\n",
      "      BatchNorm3d-33          [1, 512, 8, 8, 8]           1,024\n",
      "             ReLU-34          [1, 512, 8, 8, 8]               0\n",
      "       DoubleConv-35          [1, 512, 8, 8, 8]               0\n",
      "  ConvTranspose3d-36       [1, 256, 16, 16, 16]       1,048,832\n",
      "           Conv3d-37       [1, 256, 16, 16, 16]       3,539,200\n",
      "      BatchNorm3d-38       [1, 256, 16, 16, 16]             512\n",
      "             ReLU-39       [1, 256, 16, 16, 16]               0\n",
      "           Conv3d-40       [1, 256, 16, 16, 16]       1,769,728\n",
      "      BatchNorm3d-41       [1, 256, 16, 16, 16]             512\n",
      "             ReLU-42       [1, 256, 16, 16, 16]               0\n",
      "       DoubleConv-43       [1, 256, 16, 16, 16]               0\n",
      "  ConvTranspose3d-44       [1, 128, 32, 32, 32]         262,272\n",
      "           Conv3d-45       [1, 128, 32, 32, 32]         884,864\n",
      "      BatchNorm3d-46       [1, 128, 32, 32, 32]             256\n",
      "             ReLU-47       [1, 128, 32, 32, 32]               0\n",
      "           Conv3d-48       [1, 128, 32, 32, 32]         442,496\n",
      "      BatchNorm3d-49       [1, 128, 32, 32, 32]             256\n",
      "             ReLU-50       [1, 128, 32, 32, 32]               0\n",
      "       DoubleConv-51       [1, 128, 32, 32, 32]               0\n",
      "  ConvTranspose3d-52        [1, 64, 64, 64, 64]          65,600\n",
      "           Conv3d-53        [1, 64, 64, 64, 64]         221,248\n",
      "      BatchNorm3d-54        [1, 64, 64, 64, 64]             128\n",
      "             ReLU-55        [1, 64, 64, 64, 64]               0\n",
      "           Conv3d-56        [1, 64, 64, 64, 64]         110,656\n",
      "      BatchNorm3d-57        [1, 64, 64, 64, 64]             128\n",
      "             ReLU-58        [1, 64, 64, 64, 64]               0\n",
      "       DoubleConv-59        [1, 64, 64, 64, 64]               0\n",
      "  ConvTranspose3d-60     [1, 32, 128, 128, 128]          16,416\n",
      "           Conv3d-61     [1, 32, 128, 128, 128]          55,328\n",
      "      BatchNorm3d-62     [1, 32, 128, 128, 128]              64\n",
      "             ReLU-63     [1, 32, 128, 128, 128]               0\n",
      "           Conv3d-64     [1, 32, 128, 128, 128]          27,680\n",
      "      BatchNorm3d-65     [1, 32, 128, 128, 128]              64\n",
      "             ReLU-66     [1, 32, 128, 128, 128]               0\n",
      "       DoubleConv-67     [1, 32, 128, 128, 128]               0\n",
      "           Conv3d-68      [1, 4, 128, 128, 128]           3,460\n",
      "      BatchNorm3d-69      [1, 4, 128, 128, 128]               8\n",
      "             ReLU-70      [1, 4, 128, 128, 128]               0\n",
      "           Conv3d-71      [1, 4, 128, 128, 128]             436\n",
      "      BatchNorm3d-72      [1, 4, 128, 128, 128]               8\n",
      "             ReLU-73      [1, 4, 128, 128, 128]               0\n",
      "       DoubleConv-74      [1, 4, 128, 128, 128]               0\n",
      "          Softmax-75      [1, 4, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 22,587,688\n",
      "Trainable params: 22,587,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 32.00\n",
      "Forward/backward pass size (MB): 10726.00\n",
      "Params size (MB): 86.17\n",
      "Estimated Total Size (MB): 10844.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet3d_bn(in_channels=4, out_channels=4)\n",
    "input_tensor = torch.randn([1, 4, 128, 128, 128]).float()\n",
    "\n",
    "model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "\n",
    "out = model(input_tensor)\n",
    "print(out.shape)\n",
    "summary(model, (4, 128, 128, 128), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple UNet3d_ln\n",
    "class UNet3d_ln(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3d_ln, self).__init__()\n",
    "        self.encoder1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.encoder2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.encoder3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.encoder4 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
    "        self.encoder5 = nn.Conv3d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.decoder1 = nn.Conv3d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv_trans1 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Conv3d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv_trans2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = nn.Conv3d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv_trans3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder4 = nn.Conv3d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv_trans4 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 编码器\n",
    "        out = self.encoder1(x)                                              # 32 x 128 x 128 x 128\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        t1 = out                                                            # 32 x 128 x 128 x 128\n",
    "        \n",
    "        out = F.max_pool3d(t1, 2, 2)                                        # 32 x 64 x 64 x 64\n",
    "        out = self.encoder2(out)                                            # 64 x 64 x 64 x 64\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t2 = out                                                            # 64 x 64 x 64 x 64\n",
    "\n",
    "        out = F.max_pool3d(t2, 2, 2)                                        # 64 x 32 x 32 x 32\n",
    "        out = self.encoder3(out)                                            # 128 x 32 x 32 x 32\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t3 = out                                                            # 128 x 32 x 32 x 32\n",
    "\n",
    "        out = F.max_pool3d(t3, 2, 2)                                        # 128 x 16 x 16 x 16\n",
    "        out = self.encoder4(out)                                            # 256 x 16 x 16 x 16\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        t4 = out                                                            # 256 x 16 x 16 x 16\n",
    "        \n",
    "        out = F.max_pool3d(t4, 2, 2)                                        # 256 x 8 x 8 x 8\n",
    "        out = self.encoder5(out)                                            # 512 x 8 x 8 x 8\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        # 解码器\n",
    "        out = self.conv_trans1(out)                                         # 256 x 16 x 16 x 16\n",
    "        out = self.decoder1(torch.cat([out, t4], dim=1))                    # 256 x 16 x 16 x 16\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        out = self.conv_trans2(out)                                         # 128 x 32 x 32 x 32\n",
    "        out = self.decoder2(torch.cat([out, t3], dim=1))                    # 128 x 32 x 32 x 32\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "\n",
    "        out = self.conv_trans3(out)                                         # 64 x 64 x 64 x 64\n",
    "        out = self.decoder3(torch.cat([out, t2], dim=1))                    # 64 x 64 x 64 x 64\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                 \n",
    "        \n",
    "        out = self.conv_trans4(out)                                         # 32 x 128 x 128 x 128\n",
    "        out = self.decoder4(torch.cat([out, t1], dim=1))                    # 32 x 128 x 128 x 128\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        \n",
    "        out = self.out_conv(out)                                            # out_channels x 128 x 128 x 128\n",
    "        \n",
    "        out = self.soft(out)                                                # softmax\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 改进\n",
    "class UNet3d_ln_double(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3d_ln_double, self).__init__()\n",
    "        self.encoder1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.encoder2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.encoder3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.encoder4 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
    "        self.encoder5 = nn.Conv3d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv_32    = nn.Conv3d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv_64    = nn.Conv3d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv_128    = nn.Conv3d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv_256    = nn.Conv3d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv_512    = nn.Conv3d(512, 512, kernel_size=3, padding=1)    \n",
    "        \n",
    "        self.decoder1 = nn.Conv3d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv_trans1 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Conv3d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv_trans2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = nn.Conv3d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv_trans3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder4 = nn.Conv3d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv_trans4 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 编码器\n",
    "        out = self.encoder1(x)                                              # 32 x 128 x 128 x 128\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        out = self.conv_32(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t1 = out                                                            # 32 x 128 x 128 x 128\n",
    "        \n",
    "        out = F.max_pool3d(t1, 2, 2)                                        # 32 x 64 x 64 x 64\n",
    "        out = self.encoder2(out)                                            # 64 x 64 x 64 x 64\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_64(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t2 = out                                                            # 64 x 64 x 64 x 64\n",
    "\n",
    "        out = F.max_pool3d(t2, 2, 2)                                        # 64 x 32 x 32 x 32\n",
    "        out = self.encoder3(out)                                            # 128 x 32 x 32 x 32\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_128(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t3 = out                                                            # 128 x 32 x 32 x 32\n",
    "\n",
    "        out = F.max_pool3d(t3, 2, 2)                                        # 128 x 16 x 16 x 16\n",
    "        out = self.encoder4(out)                                            # 256 x 16 x 16 x 16\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        out = self.conv_256(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t4 = out                                                            # 256 x 16 x 16 x 16\n",
    "        \n",
    "        out = F.max_pool3d(t4, 2, 2)                                        # 256 x 8 x 8 x 8\n",
    "        out = self.encoder5(out)                                            # 512 x 8 x 8 x 8\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_512(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        # 解码器\n",
    "        out = self.conv_trans1(out)                                         # 256 x 16 x 16 x 16\n",
    "        out = self.decoder1(torch.cat([out, t4], dim=1))                    # 256 x 16 x 16 x 16\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_256(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        out = self.conv_trans2(out)                                         # 128 x 32 x 32 x 32\n",
    "        out = self.decoder2(torch.cat([out, t3], dim=1))                    # 128 x 32 x 32 x 32\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_128(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "\n",
    "        out = self.conv_trans3(out)                                         # 64 x 64 x 64 x 64\n",
    "        out = self.decoder3(torch.cat([out, t2], dim=1))                    # 64 x 64 x 64 x 64\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                 \n",
    "        out = self.conv_64(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        out = self.conv_trans4(out)                                         # 32 x 128 x 128 x 128\n",
    "        out = self.decoder4(torch.cat([out, t1], dim=1))                    # 32 x 128 x 128 x 128\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        out = self.conv_32(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        out = self.out_conv(out)                                            # out_channels x 128 x 128 x 128\n",
    "        \n",
    "        out = self.soft(out)                                                # softmax\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128, 128, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]           3,488\n",
      "            Conv3d-2       [-1, 64, 64, 64, 64]          55,360\n",
      "            Conv3d-3      [-1, 128, 32, 32, 32]         221,312\n",
      "            Conv3d-4      [-1, 256, 16, 16, 16]         884,992\n",
      "            Conv3d-5         [-1, 512, 8, 8, 8]       3,539,456\n",
      "   ConvTranspose3d-6      [-1, 256, 16, 16, 16]       1,048,832\n",
      "            Conv3d-7      [-1, 256, 16, 16, 16]       3,539,200\n",
      "   ConvTranspose3d-8      [-1, 128, 32, 32, 32]         262,272\n",
      "            Conv3d-9      [-1, 128, 32, 32, 32]         884,864\n",
      "  ConvTranspose3d-10       [-1, 64, 64, 64, 64]          65,600\n",
      "           Conv3d-11       [-1, 64, 64, 64, 64]         221,248\n",
      "  ConvTranspose3d-12    [-1, 32, 128, 128, 128]          16,416\n",
      "           Conv3d-13    [-1, 32, 128, 128, 128]          55,328\n",
      "           Conv3d-14     [-1, 4, 128, 128, 128]           3,460\n",
      "          Softmax-15     [-1, 4, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 10,801,828\n",
      "Trainable params: 10,801,828\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 32.00\n",
      "Forward/backward pass size (MB): 2170.00\n",
      "Params size (MB): 41.21\n",
      "Estimated Total Size (MB): 2243.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet3d_ln(in_channels=4, out_channels=4)\n",
    "input_tensor = torch.randn([1, 4, 128, 128, 128]).float()\n",
    "\n",
    "model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "\n",
    "out = model(input_tensor)\n",
    "print(out.shape)\n",
    "summary(model, (4, 128, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128, 128, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]           3,488\n",
      "            Conv3d-2    [-1, 32, 128, 128, 128]          27,680\n",
      "            Conv3d-3       [-1, 64, 64, 64, 64]          55,360\n",
      "            Conv3d-4       [-1, 64, 64, 64, 64]         110,656\n",
      "            Conv3d-5      [-1, 128, 32, 32, 32]         221,312\n",
      "            Conv3d-6      [-1, 128, 32, 32, 32]         442,496\n",
      "            Conv3d-7      [-1, 256, 16, 16, 16]         884,992\n",
      "            Conv3d-8      [-1, 256, 16, 16, 16]       1,769,728\n",
      "            Conv3d-9         [-1, 512, 8, 8, 8]       3,539,456\n",
      "           Conv3d-10         [-1, 512, 8, 8, 8]       7,078,400\n",
      "  ConvTranspose3d-11      [-1, 256, 16, 16, 16]       1,048,832\n",
      "           Conv3d-12      [-1, 256, 16, 16, 16]       3,539,200\n",
      "           Conv3d-13      [-1, 256, 16, 16, 16]       1,769,728\n",
      "  ConvTranspose3d-14      [-1, 128, 32, 32, 32]         262,272\n",
      "           Conv3d-15      [-1, 128, 32, 32, 32]         884,864\n",
      "           Conv3d-16      [-1, 128, 32, 32, 32]         442,496\n",
      "  ConvTranspose3d-17       [-1, 64, 64, 64, 64]          65,600\n",
      "           Conv3d-18       [-1, 64, 64, 64, 64]         221,248\n",
      "           Conv3d-19       [-1, 64, 64, 64, 64]         110,656\n",
      "  ConvTranspose3d-20    [-1, 32, 128, 128, 128]          16,416\n",
      "           Conv3d-21    [-1, 32, 128, 128, 128]          55,328\n",
      "           Conv3d-22    [-1, 32, 128, 128, 128]          27,680\n",
      "           Conv3d-23     [-1, 4, 128, 128, 128]           3,460\n",
      "          Softmax-24     [-1, 4, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 22,581,348\n",
      "Trainable params: 22,581,348\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 32.00\n",
      "Forward/backward pass size (MB): 3532.00\n",
      "Params size (MB): 86.14\n",
      "Estimated Total Size (MB): 3650.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet3d_ln_double(in_channels=4, out_channels=4)\n",
    "input_tensor = torch.randn([1, 4, 128, 128, 128]).float()\n",
    "\n",
    "model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "\n",
    "out = model(input_tensor)\n",
    "print(out.shape)\n",
    "summary(model, (4, 128, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _make_conv_layer(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, use_bn=True, use_ln=False, use_dropout=False, dropout_rate=0, ln_spatial_shape:list=[]):\n",
    "        super(_make_conv_layer, self).__init__()\n",
    "        # 参数\n",
    "        self.use_bn = use_bn\n",
    "        self.use_ln = use_ln\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.ln_spatial_shape = ln_spatial_shape\n",
    "\n",
    "        # 卷积层\n",
    "        if use_bn:\n",
    "            self.conv3x3 = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        elif use_ln:\n",
    "            self.conv3x3 = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.LayerNorm([out_channels, *ln_spatial_shape]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.LayerNorm([out_channels, *ln_spatial_shape]),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            raise\"Error: no normalization layer is used!\"\n",
    "\n",
    "        self.dropout = nn.Dropout3d(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv3x3(x)\n",
    "        if self.use_dropout:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class _make_upsample_layer(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, use_bn=True, use_ln=False, use_dropout=False, dropout_rate=0, ln_spatial_shape:list=[]):\n",
    "        super(_make_upsample_layer, self).__init__()    \n",
    "        self.use_bn = use_bn\n",
    "        self.use_ln = use_ln\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.ln_spatial_shape = ln_spatial_shape\n",
    "\n",
    "        if use_bn:\n",
    "            self.up2times = nn.Sequential(\n",
    "                nn.ConvTranspose3d(in_channels, in_channels, kernel_size=2, stride=2),\n",
    "                nn.BatchNorm3d(in_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        elif use_ln:\n",
    "            self.up2times = nn.Sequential(\n",
    "                nn.ConvTranspose3d(in_channels, in_channels, kernel_size=2, stride=2),\n",
    "                nn.LayerNorm([in_channels, *(2*ln_spatial_shape)]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.LayerNorm([out_channels, *(2*ln_spatial_shape)]),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            raise\"Error: no normalization layer is used!\"\n",
    "        \n",
    "        self.dropout = nn.Dropout3d(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.up2times(x)\n",
    "        if self.use_dropout:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, dropout_rate:float=0, use_bn:bool=True, use_ln:bool=False, use_dropout:bool=False, ln_spatial_shape:list=[]):\n",
    "        super(UNet3D, self).__init__()     \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder_use_list = (use_bn, use_ln, True, 0.1)\n",
    "        self.decoder_use_list = (use_bn, use_ln, False, 0.1)\n",
    "        # 编码器\n",
    "        self.encoder1 = _make_conv_layer(in_channels, 32, * self.encoder_use_list)\n",
    "        self.encoder2 = _make_conv_layer(32, 64, *self.encoder_use_list)\n",
    "        self.encoder3 = _make_conv_layer(64, 128, *self.encoder_use_list)\n",
    "        self.encoder4 = _make_conv_layer(128, 256, *self.encoder_use_list)\n",
    "        self.encoder5 = _make_conv_layer(256, 512, *self.encoder_use_list)\n",
    "\n",
    "        # 解码器\n",
    "        self.decoder1 = _make_conv_layer(512, 256, *self.decoder_use_list)\n",
    "        self.up1      = _make_upsample_layer(512, 256, *self.decoder_use_list)\n",
    "        self.decoder2 = _make_conv_layer(256, 128, *self.decoder_use_list)\n",
    "        self.up2      = _make_upsample_layer(256, 128, *self.decoder_use_list)\n",
    "        self.decoder3 = _make_conv_layer(128, 64, *self.decoder_use_list)\n",
    "        self.up3      = _make_upsample_layer(128, 64, *self.decoder_use_list)\n",
    "        self.decoder4 = _make_conv_layer(64, 32, *self.decoder_use_list)\n",
    "        self.up4      = _make_upsample_layer(64, 32, *self.decoder_use_list)\n",
    "\n",
    "        # 输出层\n",
    "        self.output_conv = nn.Conv3d(32, out_channels, kernel_size=1)\n",
    "\n",
    "        # 归一化层\n",
    "        self.dropout = nn.Dropout3d(dropout_rate)\n",
    "        \n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码器\n",
    "        t1 = self.encoder1(x)                                                                   # [1, 32, 128, 128, 128]\n",
    "        t2 = self.encoder2(F.max_pool3d(t1, 2, 2))                                              # [1, 64, 64, 64, 64] \n",
    "        t3 = self.encoder3(F.max_pool3d(t2, 2, 2))                                              # [1, 128, 32, 32, 32]\n",
    "        t4 = self.encoder4(F.max_pool3d(t3, 2, 2))                                              # [1, 256, 16, 16, 16]\n",
    "        out = self.encoder5(F.max_pool3d(t4, 2, 2))                                             # [1, 512, 8, 8, 8]\n",
    "\n",
    "        # Dropout\n",
    "        if self.dropout_rate > 0:\n",
    "            out = self.dropout(out)                                                              # [1, 512, 8, 8, 8]\n",
    "        # 解码器        \n",
    "        out = self.decoder1(torch.cat([self.up1(out), t4], dim=1))                               # [1, 256, 16, 16, 16]\n",
    "        out = self.decoder2(torch.cat([self.up2(out), t3], dim=1))                               # [1, 128, 32, 32, 32]                      \n",
    "        out = self.decoder3(torch.cat([self.up3(out), t2], dim=1))                               # [1, 64, 64, 64, 64]\n",
    "        out = self.decoder4(torch.cat([self.up4(out), t1], dim=1))                               # [1, 32, 128, 128, 128]\n",
    "\n",
    "        # 输出层\n",
    "        out = self.output_conv(out)\n",
    "        out = self.soft(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1     [2, 32, 128, 128, 128]           3,488\n",
      "       BatchNorm3d-2     [2, 32, 128, 128, 128]              64\n",
      "              ReLU-3     [2, 32, 128, 128, 128]               0\n",
      "            Conv3d-4     [2, 32, 128, 128, 128]          27,680\n",
      "       BatchNorm3d-5     [2, 32, 128, 128, 128]              64\n",
      "              ReLU-6     [2, 32, 128, 128, 128]               0\n",
      "         Dropout3d-7     [2, 32, 128, 128, 128]               0\n",
      "  _make_conv_layer-8     [2, 32, 128, 128, 128]               0\n",
      "            Conv3d-9        [2, 64, 64, 64, 64]          55,360\n",
      "      BatchNorm3d-10        [2, 64, 64, 64, 64]             128\n",
      "             ReLU-11        [2, 64, 64, 64, 64]               0\n",
      "           Conv3d-12        [2, 64, 64, 64, 64]         110,656\n",
      "      BatchNorm3d-13        [2, 64, 64, 64, 64]             128\n",
      "             ReLU-14        [2, 64, 64, 64, 64]               0\n",
      "        Dropout3d-15        [2, 64, 64, 64, 64]               0\n",
      " _make_conv_layer-16        [2, 64, 64, 64, 64]               0\n",
      "           Conv3d-17       [2, 128, 32, 32, 32]         221,312\n",
      "      BatchNorm3d-18       [2, 128, 32, 32, 32]             256\n",
      "             ReLU-19       [2, 128, 32, 32, 32]               0\n",
      "           Conv3d-20       [2, 128, 32, 32, 32]         442,496\n",
      "      BatchNorm3d-21       [2, 128, 32, 32, 32]             256\n",
      "             ReLU-22       [2, 128, 32, 32, 32]               0\n",
      "        Dropout3d-23       [2, 128, 32, 32, 32]               0\n",
      " _make_conv_layer-24       [2, 128, 32, 32, 32]               0\n",
      "           Conv3d-25       [2, 256, 16, 16, 16]         884,992\n",
      "      BatchNorm3d-26       [2, 256, 16, 16, 16]             512\n",
      "             ReLU-27       [2, 256, 16, 16, 16]               0\n",
      "           Conv3d-28       [2, 256, 16, 16, 16]       1,769,728\n",
      "      BatchNorm3d-29       [2, 256, 16, 16, 16]             512\n",
      "             ReLU-30       [2, 256, 16, 16, 16]               0\n",
      "        Dropout3d-31       [2, 256, 16, 16, 16]               0\n",
      " _make_conv_layer-32       [2, 256, 16, 16, 16]               0\n",
      "           Conv3d-33          [2, 512, 8, 8, 8]       3,539,456\n",
      "      BatchNorm3d-34          [2, 512, 8, 8, 8]           1,024\n",
      "             ReLU-35          [2, 512, 8, 8, 8]               0\n",
      "           Conv3d-36          [2, 512, 8, 8, 8]       7,078,400\n",
      "      BatchNorm3d-37          [2, 512, 8, 8, 8]           1,024\n",
      "             ReLU-38          [2, 512, 8, 8, 8]               0\n",
      "        Dropout3d-39          [2, 512, 8, 8, 8]               0\n",
      " _make_conv_layer-40          [2, 512, 8, 8, 8]               0\n",
      "  ConvTranspose3d-41       [2, 512, 16, 16, 16]       2,097,664\n",
      "      BatchNorm3d-42       [2, 512, 16, 16, 16]           1,024\n",
      "             ReLU-43       [2, 512, 16, 16, 16]               0\n",
      "           Conv3d-44       [2, 256, 16, 16, 16]       3,539,200\n",
      "      BatchNorm3d-45       [2, 256, 16, 16, 16]             512\n",
      "             ReLU-46       [2, 256, 16, 16, 16]               0\n",
      "_make_upsample_layer-47       [2, 256, 16, 16, 16]               0\n",
      "           Conv3d-48       [2, 256, 16, 16, 16]       3,539,200\n",
      "      BatchNorm3d-49       [2, 256, 16, 16, 16]             512\n",
      "             ReLU-50       [2, 256, 16, 16, 16]               0\n",
      "           Conv3d-51       [2, 256, 16, 16, 16]       1,769,728\n",
      "      BatchNorm3d-52       [2, 256, 16, 16, 16]             512\n",
      "             ReLU-53       [2, 256, 16, 16, 16]               0\n",
      " _make_conv_layer-54       [2, 256, 16, 16, 16]               0\n",
      "  ConvTranspose3d-55       [2, 256, 32, 32, 32]         524,544\n",
      "      BatchNorm3d-56       [2, 256, 32, 32, 32]             512\n",
      "             ReLU-57       [2, 256, 32, 32, 32]               0\n",
      "           Conv3d-58       [2, 128, 32, 32, 32]         884,864\n",
      "      BatchNorm3d-59       [2, 128, 32, 32, 32]             256\n",
      "             ReLU-60       [2, 128, 32, 32, 32]               0\n",
      "_make_upsample_layer-61       [2, 128, 32, 32, 32]               0\n",
      "           Conv3d-62       [2, 128, 32, 32, 32]         884,864\n",
      "      BatchNorm3d-63       [2, 128, 32, 32, 32]             256\n",
      "             ReLU-64       [2, 128, 32, 32, 32]               0\n",
      "           Conv3d-65       [2, 128, 32, 32, 32]         442,496\n",
      "      BatchNorm3d-66       [2, 128, 32, 32, 32]             256\n",
      "             ReLU-67       [2, 128, 32, 32, 32]               0\n",
      " _make_conv_layer-68       [2, 128, 32, 32, 32]               0\n",
      "  ConvTranspose3d-69       [2, 128, 64, 64, 64]         131,200\n",
      "      BatchNorm3d-70       [2, 128, 64, 64, 64]             256\n",
      "             ReLU-71       [2, 128, 64, 64, 64]               0\n",
      "           Conv3d-72        [2, 64, 64, 64, 64]         221,248\n",
      "      BatchNorm3d-73        [2, 64, 64, 64, 64]             128\n",
      "             ReLU-74        [2, 64, 64, 64, 64]               0\n",
      "_make_upsample_layer-75        [2, 64, 64, 64, 64]               0\n",
      "           Conv3d-76        [2, 64, 64, 64, 64]         221,248\n",
      "      BatchNorm3d-77        [2, 64, 64, 64, 64]             128\n",
      "             ReLU-78        [2, 64, 64, 64, 64]               0\n",
      "           Conv3d-79        [2, 64, 64, 64, 64]         110,656\n",
      "      BatchNorm3d-80        [2, 64, 64, 64, 64]             128\n",
      "             ReLU-81        [2, 64, 64, 64, 64]               0\n",
      " _make_conv_layer-82        [2, 64, 64, 64, 64]               0\n",
      "  ConvTranspose3d-83     [2, 64, 128, 128, 128]          32,832\n",
      "      BatchNorm3d-84     [2, 64, 128, 128, 128]             128\n",
      "             ReLU-85     [2, 64, 128, 128, 128]               0\n",
      "           Conv3d-86     [2, 32, 128, 128, 128]          55,328\n",
      "      BatchNorm3d-87     [2, 32, 128, 128, 128]              64\n",
      "             ReLU-88     [2, 32, 128, 128, 128]               0\n",
      "_make_upsample_layer-89     [2, 32, 128, 128, 128]               0\n",
      "           Conv3d-90     [2, 32, 128, 128, 128]          55,328\n",
      "      BatchNorm3d-91     [2, 32, 128, 128, 128]              64\n",
      "             ReLU-92     [2, 32, 128, 128, 128]               0\n",
      "           Conv3d-93     [2, 32, 128, 128, 128]          27,680\n",
      "      BatchNorm3d-94     [2, 32, 128, 128, 128]              64\n",
      "             ReLU-95     [2, 32, 128, 128, 128]               0\n",
      " _make_conv_layer-96     [2, 32, 128, 128, 128]               0\n",
      "           Conv3d-97      [2, 4, 128, 128, 128]             132\n",
      "          Softmax-98      [2, 4, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 28,680,548\n",
      "Trainable params: 28,680,548\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 64.00\n",
      "Forward/backward pass size (MB): 34288.00\n",
      "Params size (MB): 109.41\n",
      "Estimated Total Size (MB): 34461.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet3D(in_channels=4, out_channels=4)\n",
    "input_tensor = torch.randn([1, 4, 128, 128, 128]).float()\n",
    "\n",
    "model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "\n",
    "# out = model(input_tensor)\n",
    "# print(out.shape)\n",
    "summary(model, (4, 128, 128, 128), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BG', 'NCR', 'ED', 'ET']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {\n",
    "    'BG': 0, \n",
    "    'NCR' : 1,\n",
    "    'ED': 2,\n",
    "    'ET':3\n",
    "    }\n",
    "[k for k in labels.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _make_conv_layer(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, use_bn=True, use_ln=False, use_dropout=False, dropout_rate=0, ln_spatial_shape:list=[]):\n",
    "        super(_make_conv_layer, self).__init__()\n",
    "        # 参数\n",
    "        self.use_bn = use_bn\n",
    "        self.use_ln = use_ln\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.ln_spatial_shape = ln_spatial_shape\n",
    "\n",
    "        # 卷积层\n",
    "        if use_bn:\n",
    "            self.conv3x3 = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        elif use_ln:\n",
    "            self.conv3x3 = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.LayerNorm([out_channels, *ln_spatial_shape]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.LayerNorm([out_channels, *ln_spatial_shape]),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            raise\"Error: no normalization layer is used!\"\n",
    "\n",
    "        self.dropout = nn.Dropout3d(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv3x3(x)\n",
    "        if self.use_dropout:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class _make_upsample_layer(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, use_bn=True, use_ln=False, use_dropout=False, dropout_rate=0, ln_spatial_shape:list=[]):\n",
    "        super(_make_upsample_layer, self).__init__()    \n",
    "        self.use_bn = use_bn\n",
    "        self.use_ln = use_ln\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.ln_spatial_shape = ln_spatial_shape\n",
    "\n",
    "        if use_bn:\n",
    "            self.up2times = nn.Sequential(\n",
    "                nn.ConvTranspose3d(in_channels, in_channels, kernel_size=2, stride=2),\n",
    "                nn.BatchNorm3d(in_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        elif use_ln:\n",
    "            self.up2times = nn.Sequential(\n",
    "                nn.ConvTranspose3d(in_channels, in_channels, kernel_size=2, stride=2),\n",
    "                nn.LayerNorm([in_channels, *(2*ln_spatial_shape)]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.LayerNorm([out_channels, *(2*ln_spatial_shape)]),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            raise\"Error: no normalization layer is used!\"\n",
    "        \n",
    "        self.dropout = nn.Dropout3d(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.up2times(x)\n",
    "        if self.use_dropout:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, dropout_rate:float=0, use_bn:bool=True, use_ln:bool=False, use_dropout:bool=False, ln_spatial_shape:list=[]):\n",
    "        super(UNet3D, self).__init__()     \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder_use_list = (use_bn, use_ln, True, 0.1)\n",
    "        self.decoder_use_list = (use_bn, use_ln, False, 0.1)\n",
    "        # 编码器\n",
    "        self.encoder1 = _make_conv_layer(in_channels, 32, * self.encoder_use_list)\n",
    "        self.encoder2 = _make_conv_layer(32, 64, *self.encoder_use_list)\n",
    "        self.encoder3 = _make_conv_layer(64, 128, *self.encoder_use_list)\n",
    "        self.encoder4 = _make_conv_layer(128, 256, *self.encoder_use_list)\n",
    "        self.encoder5 = _make_conv_layer(256, 512, *self.encoder_use_list)\n",
    "\n",
    "        # 解码器\n",
    "        self.decoder1 = _make_conv_layer(512, 256, *self.decoder_use_list)\n",
    "        self.up1      = _make_upsample_layer(512, 256, *self.decoder_use_list)\n",
    "        self.decoder2 = _make_conv_layer(256, 128, *self.decoder_use_list)\n",
    "        self.up2      = _make_upsample_layer(256, 128, *self.decoder_use_list)\n",
    "        self.decoder3 = _make_conv_layer(128, 64, *self.decoder_use_list)\n",
    "        self.up3      = _make_upsample_layer(128, 64, *self.decoder_use_list)\n",
    "        self.decoder4 = _make_conv_layer(64, 32, *self.decoder_use_list)\n",
    "        self.up4      = _make_upsample_layer(64, 32, *self.decoder_use_list)\n",
    "\n",
    "        # 输出层\n",
    "        self.output_conv = nn.Conv3d(32, out_channels, kernel_size=1)\n",
    "\n",
    "        # 归一化层\n",
    "        self.dropout = nn.Dropout3d(dropout_rate)\n",
    "        \n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码器\n",
    "        t1 = self.encoder1(x)                                                                   # [1, 32, 128, 128, 128]\n",
    "        t2 = self.encoder2(F.max_pool3d(t1, 2, 2))                                              # [1, 64, 64, 64, 64] \n",
    "        t3 = self.encoder3(F.max_pool3d(t2, 2, 2))                                              # [1, 128, 32, 32, 32]\n",
    "        t4 = self.encoder4(F.max_pool3d(t3, 2, 2))                                              # [1, 256, 16, 16, 16]\n",
    "        out = self.encoder5(F.max_pool3d(t4, 2, 2))                                             # [1, 512, 8, 8, 8]\n",
    "\n",
    "        # Dropout\n",
    "        if self.dropout_rate > 0:\n",
    "            out = self.dropout(out)                                                              # [1, 512, 8, 8, 8]\n",
    "        # 解码器        \n",
    "        out = self.decoder1(torch.cat([self.up1(out), t4], dim=1))                               # [1, 256, 16, 16, 16]\n",
    "        out = self.decoder2(torch.cat([self.up2(out), t3], dim=1))                               # [1, 128, 32, 32, 32]                      \n",
    "        out = self.decoder3(torch.cat([self.up3(out), t2], dim=1))                               # [1, 64, 64, 64, 64]\n",
    "        out = self.decoder4(torch.cat([self.up4(out), t1], dim=1))                               # [1, 32, 128, 128, 128]\n",
    "\n",
    "        # 输出层\n",
    "        out = self.output_conv(out)\n",
    "        out = self.soft(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter('/mnt/d/AI_Research/WS-HUB/WS-segBratsWorkflow/Helium-327-SegBrats/results/2024-09-25/20-00-30/tensorBoard/UNet3D_braTS21_2024-09-25_20-00-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3D(4, 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_scalars('DiceLoss/val',\n",
    "                {'Mean':0.1,\n",
    "                    'ET': 0.2,\n",
    "                    'TC': 0.5,\n",
    "                    'WT': 0.5},\n",
    "                7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, torch.rand([1, 4, 128, 128, 128]).to(device))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (3327457341.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[48], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    mnt/d/AI_Research/WS-HUB/WS-segBratsWorkflow/Helium-327-SegBrats/results/2024-09-26/15-05-39/tensorBoard()\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "/mnt/d/AI_Research/WS-HUB/WS-segBratsWorkflow/Helium-327-SegBrats/results/2024-09-26/15-05-39/tensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/d/AI_Research/WS-HUB/WS-segBratsWorkflow/Helium-327-SegBrats/results/2024-09-26/17-23-28/checkpoints/UNet3D_braTS21_2024-09-26_17-23-29/best@epoch5_diceloss0.2831_dice0.3501_4.pth'\n",
    "split_path = path.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/AI_Research/WS-HUB/WS-segBratsWorkflow/Helium-327-SegBrats/results/2024-09-26/17-23-28/tensorBoard'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "results_dir = ('/').join(path.split('/')[:-3])\n",
    "\n",
    "results_dir= os.path.join(results_dir, 'tensorBoard')\n",
    "\n",
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/AI_Research/WS-HUB/WS-segBratsWorkflow/Helium-327-SegBrats/results/2024-09-26/15-05-39/logs/2024-09-26.log'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "results_dir = ('/').join(path.split('/')[:-1])\n",
    "logs_dir = os.path.join(results_dir, 'logs')\n",
    "logs_file_name = [file for file in os.listdir(logs_dir) if file.endswith('.log')]\n",
    "logs_path = os.path.join(logs_dir, logs_file_name[0])\n",
    "logs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
