{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于UNet3d进行改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "         )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 UNet_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3d_bn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3d_bn, self).__init__()\n",
    "        self.encoder1 = DoubleConv(in_channels, 32)\n",
    "        self.encoder2 = DoubleConv(32, 64)\n",
    "        self.encoder3 = DoubleConv(64, 128)\n",
    "        self.encoder4 = DoubleConv(128, 256)\n",
    "        self.encoder5 = DoubleConv(256, 512) \n",
    "\n",
    "        self.decoder1 = DoubleConv(512, 256)\n",
    "        self.conv_trans1 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(256, 128)\n",
    "        self.conv_trans2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv(128, 64)\n",
    "        self.conv_trans3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv(64, 32)\n",
    "        self.conv_trans4 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.out_conv = DoubleConv(32, out_channels)\n",
    "\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 编码器部分\n",
    "        t1 = self.encoder1(x)                                               # 32 x 128 x 128 x 128\n",
    "        out = F.max_pool3d(t1, 2, 2)                                        # 32 x 64 x 64 x 64\n",
    "                                    \n",
    "        t2 = self.encoder2(out)                                             # 64 x 64 x 64 x 64\n",
    "        out = F.max_pool3d(t2, 2, 2)                                        # 64 x 32 x 32 x 32\n",
    "        \n",
    "        t3 = self.encoder3(out)                                             # 128 x 32 x 32 x 32\n",
    "        out = F.max_pool3d(t3, 2, 2)                                        # 128 x 16 x 16 x 16\n",
    "        \n",
    "        t4 = self.encoder4(out)                                             # 256 x 16 x 16 x 16\n",
    "        out = F.max_pool3d(t4, 2, 2)                                        # 256 x 8 x 8 x 8\n",
    "        \n",
    "        out = self.encoder5(out)                                            # 512 x 8 x 8 x 8\n",
    "        \n",
    "        \n",
    "        \n",
    "        out = self.conv_trans1(out)                                         # 256 x 16 x 16 x 16\n",
    "        out = self.decoder1(torch.cat([out, t4], dim=1))                    # 256 x 16 x 16 x 16\n",
    "        \n",
    "        out = self.conv_trans2(out)                                          # 128 x 32 x 32 x 32\n",
    "        out = self.decoder2(torch.cat([out, t3], dim=1))                    # 128 x 32 x 32 x 32\n",
    "        \n",
    "        out = self.conv_trans3(out)                                         # 64 x 64 x 64 x 64\n",
    "        out = self.decoder3(torch.cat([out, t2], dim=1))                    # 64 x 64 x 64 x 64                \n",
    "\n",
    "        out = self.conv_trans4(out)                                         # 32 x 128 x 128 x 128\n",
    "        out = self.decoder4(torch.cat([out, t1], dim=1))                    # 32 x 128 x 128 x 128\n",
    "\n",
    "        out = self.out_conv(out)                                            # out_channels x 128 x 128\n",
    "        \n",
    "        out = self.soft(out)                                             # softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128, 128, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]           3,488\n",
      "       BatchNorm3d-2    [-1, 32, 128, 128, 128]              64\n",
      "              ReLU-3    [-1, 32, 128, 128, 128]               0\n",
      "            Conv3d-4    [-1, 32, 128, 128, 128]          27,680\n",
      "       BatchNorm3d-5    [-1, 32, 128, 128, 128]              64\n",
      "              ReLU-6    [-1, 32, 128, 128, 128]               0\n",
      "        DoubleConv-7    [-1, 32, 128, 128, 128]               0\n",
      "            Conv3d-8       [-1, 64, 64, 64, 64]          55,360\n",
      "       BatchNorm3d-9       [-1, 64, 64, 64, 64]             128\n",
      "             ReLU-10       [-1, 64, 64, 64, 64]               0\n",
      "           Conv3d-11       [-1, 64, 64, 64, 64]         110,656\n",
      "      BatchNorm3d-12       [-1, 64, 64, 64, 64]             128\n",
      "             ReLU-13       [-1, 64, 64, 64, 64]               0\n",
      "       DoubleConv-14       [-1, 64, 64, 64, 64]               0\n",
      "           Conv3d-15      [-1, 128, 32, 32, 32]         221,312\n",
      "      BatchNorm3d-16      [-1, 128, 32, 32, 32]             256\n",
      "             ReLU-17      [-1, 128, 32, 32, 32]               0\n",
      "           Conv3d-18      [-1, 128, 32, 32, 32]         442,496\n",
      "      BatchNorm3d-19      [-1, 128, 32, 32, 32]             256\n",
      "             ReLU-20      [-1, 128, 32, 32, 32]               0\n",
      "       DoubleConv-21      [-1, 128, 32, 32, 32]               0\n",
      "           Conv3d-22      [-1, 256, 16, 16, 16]         884,992\n",
      "      BatchNorm3d-23      [-1, 256, 16, 16, 16]             512\n",
      "             ReLU-24      [-1, 256, 16, 16, 16]               0\n",
      "           Conv3d-25      [-1, 256, 16, 16, 16]       1,769,728\n",
      "      BatchNorm3d-26      [-1, 256, 16, 16, 16]             512\n",
      "             ReLU-27      [-1, 256, 16, 16, 16]               0\n",
      "       DoubleConv-28      [-1, 256, 16, 16, 16]               0\n",
      "           Conv3d-29         [-1, 512, 8, 8, 8]       3,539,456\n",
      "      BatchNorm3d-30         [-1, 512, 8, 8, 8]           1,024\n",
      "             ReLU-31         [-1, 512, 8, 8, 8]               0\n",
      "           Conv3d-32         [-1, 512, 8, 8, 8]       7,078,400\n",
      "      BatchNorm3d-33         [-1, 512, 8, 8, 8]           1,024\n",
      "             ReLU-34         [-1, 512, 8, 8, 8]               0\n",
      "       DoubleConv-35         [-1, 512, 8, 8, 8]               0\n",
      "  ConvTranspose3d-36      [-1, 256, 16, 16, 16]       1,048,832\n",
      "           Conv3d-37      [-1, 256, 16, 16, 16]       3,539,200\n",
      "      BatchNorm3d-38      [-1, 256, 16, 16, 16]             512\n",
      "             ReLU-39      [-1, 256, 16, 16, 16]               0\n",
      "           Conv3d-40      [-1, 256, 16, 16, 16]       1,769,728\n",
      "      BatchNorm3d-41      [-1, 256, 16, 16, 16]             512\n",
      "             ReLU-42      [-1, 256, 16, 16, 16]               0\n",
      "       DoubleConv-43      [-1, 256, 16, 16, 16]               0\n",
      "  ConvTranspose3d-44      [-1, 128, 32, 32, 32]         262,272\n",
      "           Conv3d-45      [-1, 128, 32, 32, 32]         884,864\n",
      "      BatchNorm3d-46      [-1, 128, 32, 32, 32]             256\n",
      "             ReLU-47      [-1, 128, 32, 32, 32]               0\n",
      "           Conv3d-48      [-1, 128, 32, 32, 32]         442,496\n",
      "      BatchNorm3d-49      [-1, 128, 32, 32, 32]             256\n",
      "             ReLU-50      [-1, 128, 32, 32, 32]               0\n",
      "       DoubleConv-51      [-1, 128, 32, 32, 32]               0\n",
      "  ConvTranspose3d-52       [-1, 64, 64, 64, 64]          65,600\n",
      "           Conv3d-53       [-1, 64, 64, 64, 64]         221,248\n",
      "      BatchNorm3d-54       [-1, 64, 64, 64, 64]             128\n",
      "             ReLU-55       [-1, 64, 64, 64, 64]               0\n",
      "           Conv3d-56       [-1, 64, 64, 64, 64]         110,656\n",
      "      BatchNorm3d-57       [-1, 64, 64, 64, 64]             128\n",
      "             ReLU-58       [-1, 64, 64, 64, 64]               0\n",
      "       DoubleConv-59       [-1, 64, 64, 64, 64]               0\n",
      "  ConvTranspose3d-60    [-1, 32, 128, 128, 128]          16,416\n",
      "           Conv3d-61    [-1, 32, 128, 128, 128]          55,328\n",
      "      BatchNorm3d-62    [-1, 32, 128, 128, 128]              64\n",
      "             ReLU-63    [-1, 32, 128, 128, 128]               0\n",
      "           Conv3d-64    [-1, 32, 128, 128, 128]          27,680\n",
      "      BatchNorm3d-65    [-1, 32, 128, 128, 128]              64\n",
      "             ReLU-66    [-1, 32, 128, 128, 128]               0\n",
      "       DoubleConv-67    [-1, 32, 128, 128, 128]               0\n",
      "           Conv3d-68     [-1, 4, 128, 128, 128]           3,460\n",
      "      BatchNorm3d-69     [-1, 4, 128, 128, 128]               8\n",
      "             ReLU-70     [-1, 4, 128, 128, 128]               0\n",
      "           Conv3d-71     [-1, 4, 128, 128, 128]             436\n",
      "      BatchNorm3d-72     [-1, 4, 128, 128, 128]               8\n",
      "             ReLU-73     [-1, 4, 128, 128, 128]               0\n",
      "       DoubleConv-74     [-1, 4, 128, 128, 128]               0\n",
      "          Softmax-75     [-1, 4, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 22,587,688\n",
      "Trainable params: 22,587,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 32.00\n",
      "Forward/backward pass size (MB): 10726.00\n",
      "Params size (MB): 86.17\n",
      "Estimated Total Size (MB): 10844.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet3d_bn(in_channels=4, out_channels=4)\n",
    "input_tensor = torch.randn([1, 4, 128, 128, 128]).float()\n",
    "\n",
    "model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "\n",
    "out = model(input_tensor)\n",
    "print(out.shape)\n",
    "summary(model, (4, 128, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple UNet3d_ln\n",
    "class UNet3d_ln(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3d_ln, self).__init__()\n",
    "        self.encoder1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.encoder2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.encoder3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.encoder4 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
    "        self.encoder5 = nn.Conv3d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.decoder1 = nn.Conv3d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv_trans1 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Conv3d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv_trans2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = nn.Conv3d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv_trans3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder4 = nn.Conv3d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv_trans4 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 编码器\n",
    "        out = self.encoder1(x)                                              # 32 x 128 x 128 x 128\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        t1 = out                                                            # 32 x 128 x 128 x 128\n",
    "        \n",
    "        out = F.max_pool3d(t1, 2, 2)                                        # 32 x 64 x 64 x 64\n",
    "        out = self.encoder2(out)                                            # 64 x 64 x 64 x 64\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t2 = out                                                            # 64 x 64 x 64 x 64\n",
    "\n",
    "        out = F.max_pool3d(t2, 2, 2)                                        # 64 x 32 x 32 x 32\n",
    "        out = self.encoder3(out)                                            # 128 x 32 x 32 x 32\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t3 = out                                                            # 128 x 32 x 32 x 32\n",
    "\n",
    "        out = F.max_pool3d(t3, 2, 2)                                        # 128 x 16 x 16 x 16\n",
    "        out = self.encoder4(out)                                            # 256 x 16 x 16 x 16\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        t4 = out                                                            # 256 x 16 x 16 x 16\n",
    "        \n",
    "        out = F.max_pool3d(t4, 2, 2)                                        # 256 x 8 x 8 x 8\n",
    "        out = self.encoder5(out)                                            # 512 x 8 x 8 x 8\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        # 解码器\n",
    "        out = self.conv_trans1(out)                                         # 256 x 16 x 16 x 16\n",
    "        out = self.decoder1(torch.cat([out, t4], dim=1))                    # 256 x 16 x 16 x 16\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        out = self.conv_trans2(out)                                         # 128 x 32 x 32 x 32\n",
    "        out = self.decoder2(torch.cat([out, t3], dim=1))                    # 128 x 32 x 32 x 32\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "\n",
    "        out = self.conv_trans3(out)                                         # 64 x 64 x 64 x 64\n",
    "        out = self.decoder3(torch.cat([out, t2], dim=1))                    # 64 x 64 x 64 x 64\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                 \n",
    "        \n",
    "        out = self.conv_trans4(out)                                         # 32 x 128 x 128 x 128\n",
    "        out = self.decoder4(torch.cat([out, t1], dim=1))                    # 32 x 128 x 128 x 128\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        \n",
    "        out = self.out_conv(out)                                            # out_channels x 128 x 128 x 128\n",
    "        \n",
    "        out = self.soft(out)                                                # softmax\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 改进\n",
    "class UNet3d_ln_double(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3d_ln_double, self).__init__()\n",
    "        self.encoder1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.encoder2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.encoder3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.encoder4 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
    "        self.encoder5 = nn.Conv3d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv_32    = nn.Conv3d(32, 32, kernel_size=3, padding=1)\n",
    "        self.conv_64    = nn.Conv3d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv_128    = nn.Conv3d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv_256    = nn.Conv3d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv_512    = nn.Conv3d(512, 512, kernel_size=3, padding=1)    \n",
    "        \n",
    "        self.decoder1 = nn.Conv3d(512, 256, kernel_size=3, padding=1)\n",
    "        self.conv_trans1 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Conv3d(256, 128, kernel_size=3, padding=1)\n",
    "        self.conv_trans2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = nn.Conv3d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv_trans3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder4 = nn.Conv3d(64, 32, kernel_size=3, padding=1)\n",
    "        self.conv_trans4 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 编码器\n",
    "        out = self.encoder1(x)                                              # 32 x 128 x 128 x 128\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        out = self.conv_32(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t1 = out                                                            # 32 x 128 x 128 x 128\n",
    "        \n",
    "        out = F.max_pool3d(t1, 2, 2)                                        # 32 x 64 x 64 x 64\n",
    "        out = self.encoder2(out)                                            # 64 x 64 x 64 x 64\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_64(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t2 = out                                                            # 64 x 64 x 64 x 64\n",
    "\n",
    "        out = F.max_pool3d(t2, 2, 2)                                        # 64 x 32 x 32 x 32\n",
    "        out = self.encoder3(out)                                            # 128 x 32 x 32 x 32\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_128(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t3 = out                                                            # 128 x 32 x 32 x 32\n",
    "\n",
    "        out = F.max_pool3d(t3, 2, 2)                                        # 128 x 16 x 16 x 16\n",
    "        out = self.encoder4(out)                                            # 256 x 16 x 16 x 16\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        out = self.conv_256(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        t4 = out                                                            # 256 x 16 x 16 x 16\n",
    "        \n",
    "        out = F.max_pool3d(t4, 2, 2)                                        # 256 x 8 x 8 x 8\n",
    "        out = self.encoder5(out)                                            # 512 x 8 x 8 x 8\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_512(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        # 解码器\n",
    "        out = self.conv_trans1(out)                                         # 256 x 16 x 16 x 16\n",
    "        out = self.decoder1(torch.cat([out, t4], dim=1))                    # 256 x 16 x 16 x 16\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_256(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        out = self.conv_trans2(out)                                         # 128 x 32 x 32 x 32\n",
    "        out = self.decoder2(torch.cat([out, t3], dim=1))                    # 128 x 32 x 32 x 32\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        out = self.conv_128(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "\n",
    "        out = self.conv_trans3(out)                                         # 64 x 64 x 64 x 64\n",
    "        out = self.decoder3(torch.cat([out, t2], dim=1))                    # 64 x 64 x 64 x 64\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                 \n",
    "        out = self.conv_64(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        out = self.conv_trans4(out)                                         # 32 x 128 x 128 x 128\n",
    "        out = self.decoder4(torch.cat([out, t1], dim=1))                    # 32 x 128 x 128 x 128\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))                     \n",
    "        out = self.conv_32(out)\n",
    "        out = F.relu(F.layer_norm(out, out.shape[-3:]))\n",
    "        \n",
    "        out = self.out_conv(out)                                            # out_channels x 128 x 128 x 128\n",
    "        \n",
    "        out = self.soft(out)                                                # softmax\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128, 128, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]           3,488\n",
      "            Conv3d-2       [-1, 64, 64, 64, 64]          55,360\n",
      "            Conv3d-3      [-1, 128, 32, 32, 32]         221,312\n",
      "            Conv3d-4      [-1, 256, 16, 16, 16]         884,992\n",
      "            Conv3d-5         [-1, 512, 8, 8, 8]       3,539,456\n",
      "   ConvTranspose3d-6      [-1, 256, 16, 16, 16]       1,048,832\n",
      "            Conv3d-7      [-1, 256, 16, 16, 16]       3,539,200\n",
      "   ConvTranspose3d-8      [-1, 128, 32, 32, 32]         262,272\n",
      "            Conv3d-9      [-1, 128, 32, 32, 32]         884,864\n",
      "  ConvTranspose3d-10       [-1, 64, 64, 64, 64]          65,600\n",
      "           Conv3d-11       [-1, 64, 64, 64, 64]         221,248\n",
      "  ConvTranspose3d-12    [-1, 32, 128, 128, 128]          16,416\n",
      "           Conv3d-13    [-1, 32, 128, 128, 128]          55,328\n",
      "           Conv3d-14     [-1, 4, 128, 128, 128]           3,460\n",
      "          Softmax-15     [-1, 4, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 10,801,828\n",
      "Trainable params: 10,801,828\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 32.00\n",
      "Forward/backward pass size (MB): 2170.00\n",
      "Params size (MB): 41.21\n",
      "Estimated Total Size (MB): 2243.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet3d_ln(in_channels=4, out_channels=4)\n",
    "input_tensor = torch.randn([1, 4, 128, 128, 128]).float()\n",
    "\n",
    "model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "\n",
    "out = model(input_tensor)\n",
    "print(out.shape)\n",
    "summary(model, (4, 128, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128, 128, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1    [-1, 32, 128, 128, 128]           3,488\n",
      "            Conv3d-2    [-1, 32, 128, 128, 128]          27,680\n",
      "            Conv3d-3       [-1, 64, 64, 64, 64]          55,360\n",
      "            Conv3d-4       [-1, 64, 64, 64, 64]         110,656\n",
      "            Conv3d-5      [-1, 128, 32, 32, 32]         221,312\n",
      "            Conv3d-6      [-1, 128, 32, 32, 32]         442,496\n",
      "            Conv3d-7      [-1, 256, 16, 16, 16]         884,992\n",
      "            Conv3d-8      [-1, 256, 16, 16, 16]       1,769,728\n",
      "            Conv3d-9         [-1, 512, 8, 8, 8]       3,539,456\n",
      "           Conv3d-10         [-1, 512, 8, 8, 8]       7,078,400\n",
      "  ConvTranspose3d-11      [-1, 256, 16, 16, 16]       1,048,832\n",
      "           Conv3d-12      [-1, 256, 16, 16, 16]       3,539,200\n",
      "           Conv3d-13      [-1, 256, 16, 16, 16]       1,769,728\n",
      "  ConvTranspose3d-14      [-1, 128, 32, 32, 32]         262,272\n",
      "           Conv3d-15      [-1, 128, 32, 32, 32]         884,864\n",
      "           Conv3d-16      [-1, 128, 32, 32, 32]         442,496\n",
      "  ConvTranspose3d-17       [-1, 64, 64, 64, 64]          65,600\n",
      "           Conv3d-18       [-1, 64, 64, 64, 64]         221,248\n",
      "           Conv3d-19       [-1, 64, 64, 64, 64]         110,656\n",
      "  ConvTranspose3d-20    [-1, 32, 128, 128, 128]          16,416\n",
      "           Conv3d-21    [-1, 32, 128, 128, 128]          55,328\n",
      "           Conv3d-22    [-1, 32, 128, 128, 128]          27,680\n",
      "           Conv3d-23     [-1, 4, 128, 128, 128]           3,460\n",
      "          Softmax-24     [-1, 4, 128, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 22,581,348\n",
      "Trainable params: 22,581,348\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 32.00\n",
      "Forward/backward pass size (MB): 3532.00\n",
      "Params size (MB): 86.14\n",
      "Estimated Total Size (MB): 3650.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet3d_ln_double(in_channels=4, out_channels=4)\n",
    "input_tensor = torch.randn([1, 4, 128, 128, 128]).float()\n",
    "\n",
    "model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "\n",
    "out = model(input_tensor)\n",
    "print(out.shape)\n",
    "summary(model, (4, 128, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 128, 128, 128, [1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list = (128 ,128, 128, [1, 1, 1])\n",
    "\n",
    "b_list = (4, *a_list)\n",
    "\n",
    "b_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3090050/655014977.py:66: UserWarning: FigureCanvasSVG is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.switch_backend('SVG')\n",
    "\n",
    "# 创建一个简单的神经网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 创建一个数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.X = torch.rand(1000, 2)\n",
    "        self.y = torch.rand(1000, 2)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "# 创建一个数据加载器\n",
    "dataset = Dataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 创建一个模型和优化器\n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义动画函数\n",
    "def animate(i):\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    ax.clear()\n",
    "    ax.plot(range(i+1), [loss.item()], 'bo-')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss')\n",
    "    return ax,\n",
    "\n",
    "# 创建一个新图形\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 创建动画对象\n",
    "ani = animation.FuncAnimation(fig, animate, frames=100, interval=50, blit=True)\n",
    "\n",
    "# 显示动画\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3090050/2022803415.py:24: UserWarning: FigureCanvasSVG is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# 创建一个新的图形\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 初始化x和y值\n",
    "x = np.arange(0, 2*np.pi, 0.01)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 创建一个线条对象\n",
    "line, = ax.plot(x, y)\n",
    "\n",
    "# 定义动画函数\n",
    "def animate(i):\n",
    "    line.set_ydata(np.sin(x + i / 50))  # 更新y值\n",
    "    return line,\n",
    "\n",
    "# 创建动画对象\n",
    "ani = animation.FuncAnimation(fig, animate, frames=200, interval=20, blit=True)\n",
    "\n",
    "# 显示动画\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
